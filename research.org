* Books
** Neural Network Architectures - Judith Dayhoff
   [[http://books.google.com/books/about/Neural_network_architectures.html?id=9RwnAAAAMAAJ][Google Books]]
 
#+BEGIN_QUOTE
(p12) "Neural network architectures encode information in a distributed fashion. Typically the information that is stored in a neural net is shared by many of its processing units. This type of coding is in stark contrast to traditional memory schemes, where particular pieces of information are stored in particular locations of memory. Traditional speech recognition systems, for example, contain a lookup table of template speech patterns (individual syllables or words) that are compared one by one to spoken inputs. Such templates are stored in a specific location of the computer memory. Neural networks, in contrast, identify spoken syllables by using a number of processing units simultaneously. The internal representation is thus distributed across all or part of the network. Furthermore, more than one syllable or pattern may be stored at the same time by the same network.

    Distributed storage schemes provide many advantages, the most important being that the information representation can be redundant. Thus a neural network system can undergo partial destruction of the network and may still be able to function correctly. Although redundancy can be buit into other types of systems, the neural network has a natural way to organize and implement this redundancy; the result is a naturally fault- or err-tolerant system."
#+END_QUOTE

    Neural networks are not unlike self-organizing agent systems. Each node can be considered as an agent that is very simply programmed (responds to stimulus from the environment by receiving activations from incoming connections and using its assigned function to send an output to other agents) and collectively they are performing a desired global task. They are self-organizing in that they learn to communicate the appropriate values between one another that results in the system being able to perform well. None of the agents have knowledge of the global task or the state of the rest of the network, all communication is local (with perhaps some further reaching connections being permitted, e.g. long range feedback or the concatenation approach to handling autoencoders.

#+BEGIN_QUOTE
(p39) "Mathematical analysis has shown that when this equality is true (bi-directional weights equal), the network is able to converge - that is, it eventually attains a stable state. Convergence of the network is necessary in order for it to perform useful computational tasks such as optimizatino and associative memory. Many networks with unqual weights (w_ij != w_ji) also converge successfully.
#+END_QUOTE

#+BEGIN_QUOTE
(p42) "The randomized updates used in the Hopfield network provide an important difference between it and other paradigms. Most other neural network paradigms have a layer of processing units updated at the same time (or nearly the same time). In contrast, the asynchronous updating of the Hopfield Net is a closer match to biological reality - biological neurons update their own states due to events that impinge upon the neuron. These impinging events are not synchronized from neuron to neuron."
#+END_QUOTE

#+BEGIN_QUOTE
(p56) "The nonsynchronous updating of processing units in the Hopfield network is a unique property, and is especially relevant to the study of biological systems becuase of their asynchronous updating of nerve cell attributes. In addition, asynchronous updating can be helpful in designing fast hardware implementations.

The most general advantage of the Hopfield network is its inherently parallel architecture. As a result, potential hardware implementations may be very fast. Tradeoffs must be assessed, however, between the size and speed of the network and the size of the applications problems."
#+END_QUOTE

#+BEGIN_QUOTE
(p126) "Terminal boutons are at the ends of the axon branches. These structures, which usually form small bulges at the end of each fiber, release chemical transmitters, which cross the synaptic gap to reach a receiving neuron. When these transmitters are released, the membrane fo the target neuron is affected, and its inclination to fire its own impulse is changed - either increased or decreased, allowing an incoming signal to be either excitatory or inhibitory."
#+END_QUOTE

#+BEGIN_QUOTE
(p127) "Nerve cells come in many different sizes and shapes, and are usually tailored for the jobs that they do: Some nerve cells have tremendous arborizations of dendrites or axons, some have long branches, and others have short branches. Neurons usually create signals via impulses, but some transmit signals to other cells without an impulse. Some nerve axons are insulated with a material called myelin, and others are not. Trunk diameters of dendrites can differ from cell to cell, and the detailed structure of the axon hillock can be different, leading to different dynamics for impulse initiation."
#+END_QUOTE

#+BEGIN_QUOTE
(p142) "The waveforms of successive impulses appear the same; thus, the shape of the impulse is not important. The important aspects of the impulse train are the times of occurrences of impulses, the times of occurrence of volleys of impulses, and the temporal delays between successive impulses. Which nerve cells fire also appears to be important in the nervous system as well as groups of nerve cells that tend to fire together or in particular types of patterns."
#+END_QUOTE

#+BEGIN_QUOTE
(p142) "Traditionally, a nerve cell that transmits impulses down its axons and receives impulses at its dendrites has been considered a standard neuron, however many exceptions to this "standard" exist. Not all nerve cells generate imipulses, for example those in the retina. Those that do not generate imipulses rely on their ability to spread changes in membrane potentials along the cell membrane. These changes can cause the release of neurotransmitter at synapses, and thus affect target cells without the use of impulses."
#+END_QUOTE

#+BEGIN_QUOTE
(p154) "Summation in biological units is different from that in artificial neural networks. Artificial networks sum arriving scalar values at discrete time steps. Biological neural networks have mechanisms for temporal integration as well as summation of incoming signals, and biological mechanisms for inhibition are different from those for excitation, producing effects with differeng magnitudes and dynamics.

When a neurotransmitter is received at a postsynaptic cell, the cell's membrane is affected. The way in whihc this effect occurs is key to how the target neuron sums its inputs. The process is time modulated, spacially modulated, and influenced by the conductances and locations of ions." 
#+END_QUOTE

#+BEGIN_QUOTE
(p156) "Yet another factor enters into the summation of incoming impulses - the size and shape of the dendritic tree and its proecesses. The dendrite branching topology, the branch diameters, and the trunk size of the dendritic tree all influence the summation of incoming signals. The position of the synapse on the dendritic tree or on the cell body also influences how arriving pulses are summed. ...

A pivotal issue is identifying what makes neurons fire. Usually a number of incoming impulses are required to make a target cell fire. If the arriving impulses are spaced closely in time, then the target cell sums the EPSPs rapidly, with little time decay between arrivals. Thus, impulses that are closely spaced in time are more likely to cause the target neuron to fire. These incoming impulses could be from the same or different neurons. A group of neurons that all feed the same target neuron could trigger a response if they fire in near synchrony." 
#+END_QUOTE

#+BEGIN_QUOTE
(p157) "A question remains about the possible significance of temporal patterns in the timing of pulses from neurons and assemblies of neurons. Favored patterns - those that repeat unexpectedly - have been found, and other temporal patterns have appeared in a variety of preparations. Such patterns might have biological significance in contributing to the dynamic processing and encoding done in biological neural networks. Although a temporal pattern is not expected to elicit an individual pulse from an individual target neuron, such a pattern contributes to temporal patterns that are output from assemblies of neurons. At the least, this patterning could be a side effect of the way the nervous system processes. At most, impulse patterns and correlations could be a code for information processing and representation. It is possible that identifying and tracking assembly patterns could turn out to be a highly significant way of approaching and understanding neural dynamics.

The structure of biological neural networks is at once both divergent and convergent. The output of a neuron is fractionated, as its axon branches to synapse onto many other neurons. A single neuron in turn receives input from many sources. The temporal spacing of impulses is superimposed on this anatomical structure of divergent and convergent interconnections. Biological nervous systems, thus, can be considered to perform spatiotemporal processing, a much more complex process than the parallel or serial processing of conventional computers."
#+END_QUOTE

Could these recurring patterns she is referring to be related to dynamic attractors in the state space of the brain? It would be interesting to know if the patterns she is describing are found to adhere to definable attractor patterns."

#+BEGIN_QUOTE
(p159) "Biological systems, on the other hand, have a built-in temporal structure because impulses can occur at any time, and thus may form temporal patterns. Thu summation in the two systems [biological and artificial] must be done differently as a result of their different signaling characteristics."
#+END_QUOTE

#+BEGIN_QUOTE
(p160) "The layers in biological systems are not fully interconnected with layers above and below in the simplistic way that is found in many artificial systems, as biological connections may be sparse or amy involve more than one synapse. Three-dimensional packing considerations discourage fully interconnected topologies for biological systems, and also pose a constraint in designing neurocomputing hardware."
#+END_QUOTE

*** Architecture Types
  + Perceptron
    - 0 or 1 thresholded output
    - Real-valued weights.
    - Only one *trainable* layer of weights. Possible to have multiple layers, but additional layers beyond the one trainable layer will be fixed weights.
    - A weight only changes if its corresponding training input is non-zero and there is a difference between target and output values.
  + Adaline/Madaline (_Ada_ptive _Li_near _Ne_uron)
    - Adaline has (+/-)1 thresholded output.
    - Real-valued weights.
    - Madaline is a system of connected adalines: layer of adalines, each adaline connected to an output unit.
    - Majority vote of adalines determines output of madeline: more than half of adalines have +1(or -1), so does the madaline.
    - Weight update: adaline unit whose sum was closest to 0 in the wrong direction is updated. "the adaptiation rule assigns responsibility to the unit that can most easily assume it. Only one adaline unit updates its weights; the others keep their weights the same"
  + Hopfield Network
    - Fully interconnected network of units (complete digraph)
    - Each unit binary 0 or 1 (alternatively -1 or 1)
    - Network state represented by bit vector
    - With weights w_ij = w_ji, network converges to a stable state (all units left unchanged after update), *necessary to perform useful computational tasks*
    - Update one unit at a time, continue until no more changes are possible
    - Can update units in sequence; Hopfield chose at random to ensure same average update rate
    - [[http://cns.upf.edu/jclub/hopfield82.pdf][Hopfield (1982)]] proves network will reach a stable state.
    - Cannot reach global minimum of "Energy" function from local minimum (Boltzmann machine overcomes this with noise to shake it out)
    - [[http://www.pnas.org/content/81/10/3088.full.pdf][Hopfield (1984)]] extended the network to have continuous unit values (using nonlinear sigmoid function)
    - Traveling Salesman: can represent tours as NxN matrix with one 1 in each column and row, corresponding to one city at a time being visited only once. Energy function has multiple terms, each term imposing a constraint of the problem. Hopfield had success with <30 cities but it was shown later that the approach does not scale to larger problem spaces.
  + Back-propagation
    - Input layer, output layer, one or more hidden layers. 
    - A layer is often fully interconnected to the next layer in the stack, but not necessarily
    - No a priori knowledge of a mathematical function mapping input patterns to output patterns is needed; self-organization of the weights finds the mapping
    - Nonlinear activation function for hidden layer provides "soft" threshold
    - Standard sigmoid has "interesting" values for range [-3,3], i.e. values increase monotonically from 0 to 1 with a sharp increase around x=0; asymptotically goes to 0 for x < -3 and to 1 for x > 3
    - Sigmoid can be shifted left/right by adding/subtracting constant value - this is what the bias nodes do
    - In the error deltas for the output units \delta_j = (t_j - a_j)f^\prime(S_j), the f^\prime forces a stronger correction when S_j is near the rapid rise of the sigmoid (sigmoid derivative is a bell-shaped curve centered at 0)
    - In weight update \Delta w_{ji} = \eta \delta_j a_i, larger error \delta_j results in larger adjustments to incoming weights, larger activation a_i of originating unit from lower layer results in larger weight adjustment.
    - Learning rate \eta (usually in range [0.25, 0.75] can cause network instability if too large, and very slow training if too small.
    - A RMSE value < 0.1 indicates the training set is learned
    - Techniques for avoiding local minima and speeding up convergence:
      * change the learning rate
      * simulated annealing: start with large learning rate and attenuate its value as training proceeds
      * change number of hidden nodes 
      * add noise to weights
      * momentum 
  + Competitive Learning
    - Two layers, input layer and competitive layer, layers are fully interconnected layers
    - (p96) "In the competitive layer, the units compete for the opportunity to respond to the input pattern."
    - Input units have activations of 0 or 1, weights are small in [0,1] and sum to 1
    - For winner take all scheme, unit in output layer with highest weighted sum has activation 1, all others 0
    - Only weights going to winner are updated, and are updated by \Delta w_{ji} = g(\frac{x_i}{m} - w_{ji}) for g learning parameter (usually in range [0.01, 0.3]) and m number of active input units
    - Weight incremented when corresponding unit has activation 1, decremented when 0
    - Update procedure retains weights summing to 1
    - Unsupervised for dividing input patterns into self-learned classes
    - Can have units use fully connected inhibitory connections, so that the network gradually "chooses" the winner
    - Inhibitory activation is subtracted from unit activation
    - Lateral inhibition (inhibition of neighbors) can help in contrast and making sharper transitions in representations

* Papers
** Machine Learning
*** Deep Learning
**** DONE Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions - Socher 2011
    CLOSED: [2015-03-03 Tue 14:25]
    [[http://nlp.stanford.edu/~jpennin/papers/D11-1014.pdf][Paper PDF]]
    [[http://dl.acm.org/citation.cfm?id=2145450][Citation]]
**** DONE Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank - Socher 2013
    CLOSED: [2015-03-04 Wed 23:22]
    [[http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf][Paper PDF]]
*** NLP
**** DONE Efficient Estimation of Word Representations in Vector Space - Mikolov 2013
    CLOSED: [2015-03-07 Sat 17:43]
    [[http://arxiv.org/pdf/1301.3781.pdf][Paper PDF]]

    "Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen."

Maybe this kind of algebraic reasoning could be used to uncover causal relationships? That is, suppose you built up vector representations of your corpus such that you had a well-defined cause and effect relationship established. For example, you had a relationship "pressure increases as volume decreases". If you had compositional representations, then you could represent this in your vector space as V = vector("pressure increases") + vector("volume decreases"). If this is a cause-effect sentence based on the corpus you trained on, then you should be able to V - vector("cause") = vector("pressure increases") which would be the left over effect in the sentence. That seems to presuppose the knowledge of the causal relationship, but if your corpus contained a lot of instances of cause effect relationships, especially when established by an explicit causal connective, then maybe that would come out. 

You could maybe build up a notion of "cause" by performing some computation on words with a high similarity to "because", so then vector("cause") would be something meaningful to perform algebraic operations on no matter the causal connective actually used (because, since, as, for, thus, therefore, etc.). Presumably this kind of similarity would come out of the corpus training, and perhaps you could augment it by training on a large corpus of argumentative text (to learn the causal relations in itself) and then incorporate into your domain corpus.

**** DONE Distributed Representations of Words and Phrases and their Compositionality - Mikolov 2013
    CLOSED: [2015-03-07 Sat 18:45]
    [[http://arxiv.org/pdf/1310.4546.pdf][Paper PDF]]

    "Word representations are limited by their inability to represent idiomatic phrases that are not compositions of the individual words. For example, “Boston Globe” is a newspaper, and so it is not a natural combination of the meanings of “Boston” and “Globe”. Therefore, using vectors to represent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques that aim to represent meaning of sentences by composing the word vectors, such as the recursive autoencoders (socher), would also benefit from using phrase vectors instead of the word vectors."

Phrase composition seems to need very large data sets (they used 33 billion words) to be successful. BUT, they were using just effectively random text and learning very general concepts. Perhaps if you are operating only in a specific domain and are using training data that involves the same content but phrased in many unique ways, it might be able to pick up on phrases in that particular domain well. And generally that seems the only means by which this approach will be successful in the causality domain without resorting to an enormous training set - there will be a lot of content redundancy in the training data, but it will all be about the desired topic and so it can pick up on the causal relations within that domain. To extrapolate to other domains, will have to train on other domains, and to make this feasible in domain-free setting (i.e the domain is unknown), will likely have to train vector representations on an enormous corpus rich with causal relationships.

**** DONE Linguistic Regularities in Continuous Space Word Representations - Mikolov 2013
    CLOSED: [2015-03-07 Sat 19:08]
    [[http://research.microsoft.com/pubs/189726/rvecs.pdf][Paper PDF]]

    "In this model, to answer the analogy question a:b c:d where d is unknown, we find the embedding vectors x_a, x_b, x_c (all normalized to unit norm), and compute y = x_b − x_a + x_c. y is the continuous space representation of the word we expect to be the best answer. Of course, no word might exist at that exact position, so  we then search for the word whose embedding vector has the greatest cosine similarity to y and output it: w^* = argmax_w \frac{x_w y}{\Vert x_w \Vert \Vert y \Vert}"

**** DONE Causal Relation Extraction - Blanco 2008
    CLOSED: [2015-03-21 Sat 08:58]
    [[http://www.ece.uc.edu/~mazlack/dbm.w2010/Causal%20Text%20Networks/Blanco.2008.pdf][Paper PDF]]
**** DONE Automatic extraction of cause-effect relations in Natural Language Text - Sorgente 2013
    CLOSED: [2015-03-21 Sat 10:19]
    [[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.403.2300&rep=rep1&type=pdf#page=42][Paper PDF]]

*** Reservoir Computing
**** Notes
     In ESNs the reservoir is a general computing space - the sparse random connections provide a rich representational space that allow processing to occur in a non-programmatic way. It is a means of representing the input signal in a novel way and an output layer can be trained to learn regularities in this new representational space. It is a general computing space since no training is done in the reservoir, it is simply a rich representational space that can be used for arbitrary signal processing.

     The reservoir is also serving as a memory, which is why it is primarily useful for tasks of a temporal nature where there are dependencies existing over time. To my mind, this seems like a good reason to use it for natural language processing because language is inherently something that occurs over time and a word in one part of the sentence will have a dependency on other parts of the sentence. Though interestingly it is not necessarily the case the the dependency will occur linearly in one direction, i.e. the dependency may not necessarily be from a present word to a previous word but could be from a present word to a future word. Dependencies exist also over multiple sentences. This may be a reason consider a whole essay as an input sequence to find dependencies over sentence divisions, and for a single sentence, multiple passes might be required or inputting the sentence as a repeated signal in order to first let the entire signal be "known" to the reservoir, and then have it discern features of the sentence.

**** DONE Learning grammatical structure with Echo State Networks - Tong 2007
    CLOSED: [2015-03-28 Sat 12:30]
    [[http://cseweb.ucsd.edu/~echristiansen/papers/grammar_esn.pdf][Paper PDF]]

    Paper indicates that their work is the among the first applications of ESNs to natural language processing. They use a very, very limited vocabulary though and the grammar is too simple to approach full natural language use. They say in the final paragraph of the discussion that their results are "insufficient to indicate whether ESNs are capable of scaling to a large natural language corpus." ESNs in full natural language in 2007 was therefore uncharted territory. The promise of ESNs is indicated though since the trained paramaters scale linearly with network size, as opposed to other recurrent models that scale quadratically.

**** DONE Language modelling using augmented echo state networks - Rachez 2014
    CLOSED: [2015-03-28 Sat 16:47]
    [[http://www.ijicic.org/ijicic-13-12004.pdf][Paper PDF]]

    Used an artificial corpus of 5500 words to train, sentences automatically generated from a grammar 3-9 words in length with average length 6.7. They treat the corpus as one huge word sequence though in training.

    They also did not try their work on a full scale natural language corpus and only presented results based on the artificial corpus. The recurring task seems to be predicting the next word based on the previously seen sequence of words. I would instead like to know how an ESN can be used in teasing out particular semantic regularities in sentences, and causality specifically. The representational space of the reservior might be exploited to this end, and it could be augmented if needed, e.g. a "deep reservoir" with layered reservoirs to build up representations iteratively or with tuned readout layers to track down particular features of a sentence.

**** DONE Probabilistic Language Modeling using Echo State Networks - Rachez 2012
    CLOSED: [2015-03-29 Sun 09:33]
    [[http://www.researchgate.net/publication/236117115_Probabilistic_Language_Modeling_using_Echo_State_Networks][Paper Download]]
    [[file:../papers/rachez2012.pdf][Local PDF]]

    It seems that the input vectors utilized are one-hot word vectors. They add the feature training layer to add to the system the ability for similar sentences to cause similar trajectories through the state space of the encoded dynamical system in the reservoir. It would seem that some of this work would already be achieved by using the high dimensional pre-trained word vectors from word2vec. You could then input a sequence of words and that would presumably cause more complex dynamics in the reservoir than would have been achieved with simple one-hot vectors that are numerically simple and encode nothing about the relationships the words might have to one another. The word2vec vectors at least encode some semantic similarities between the words and maybe that could contribute to meaningful similaries in the dynamics of the reservoir.

    Their use of trained features results in a reorganization of the activations of the recurrent layer which, as shown by PCA projecting onto 2 dimensions, results in clusters of activations and more generally indicates that greater separation and differentiation is being made in the reservoir dynamics. This should make it easier to read off classifications using the output projections. This makes good sense for categorical classification, but what about a more complex readout? Is there a way to train the network such that it can learn to identify particular features of the inputs sentence? Maybe have feedback connections that loop back to the input layer that can take what is learned in the reservoir representation and pick out words or phrases in the source sentence that can be classified appropriately (e.g. it could find explicit causal keywords and ideally distinguish between cause and effect). Have it set up so that the output is some form of a tagged subset of the input.

**** TODO Recurrent temporal networks and language acquisition - Dominey 2013
    [[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3733003/pdf/fpsyg-04-00500.pdf][Paper PDF]]

**** DONE On-line processing of grammatical structure using reservoir computing - Hinaut 2012
     CLOSED: [2015-03-29 Sun 20:07]
    [[../papers/hinaut2012.pdf][Local PDF]]

**** DONE Real-time parallel processing of grammatical structure in the fronto-striatal system (reservoir computing) - Hinaut 2013
     CLOSED: [2015-03-31 Tue 11:36]
    [[../papers/hinaut2013.pdf][Local PDF]]

    "Our models are based on the principle that the information necessary to perform this thematic role assignment is encoded in the sentence by the configuration of grammatical function words (e.g. determiners, auxiliary verbs, prepositions) within the sentence. ... In our modeling, the notion is that the sequence of closed class words forms a pattern of activity within the recurrent network, and that this pattern can be associated with the corresponding thematic role specification."

    My hypothesis would then be in the same vein but for causality. That is, a sentence encodes causal relations in virtue of sentence structure and word patterns, and thus the recurrent reservoir will have dynamics that reflect this structure and can be used by readout connections to discern whether causal structure is present and possibly even discern the particulars of the causal relation (what is the cause and what is the effect).

    Nice description of reservoir computing:    
    "Reservoir computing is a machine learning technique in which a recurrent network with fixed connections is used to encode the spatiotemporal structure of an input sequence, and connections to a readout layer are trained using efficient algorithms to produce a desired output in response to input sequences. The central concept of reservoir computing is to project low dimensional inputs onto a recurrently connected "reservoir" of neurons which creates a high-dimensional projection of the low-dimensional input space, thus increasing the input seperability. The recurrent connections provide sensitivity to events over time, thus yielding the desired sensitivity to sequential and temporal organization. This projection and the dynamics of the reservoir serve as a form of kernel in the machine learning sense, thus allow one to perform complex nonlinear computational tasks with simiple linear readouts."

    IMPORTANT: they reset the reservoir state before the presentation of each input sentence

    "The reservoir encodes the ongoing trajectory of grammatical structure as words successively arrive. The trained read-out neurons extract this structure in real-time, in a predictive manner. This readout activity reflects the current probabilities for each of the multiple possible parses in parallel."

    They show that the network is indeed picking up on grammatical structure, since they tested it on a scrambled corpus that destroyed the grmamatical structure and got terrible results. The success is dependent on training on a corpus that exhibits proper grammatical structure (which doesn't bode well for the student-written essays as serving as the training corpus).

**** DONE A practical guide to applying echo state networks - Lukosevicius 2012
     CLOSED: [2015-03-29 Sun 11:12]
    [[http://organic.elis.ugent.be/sites/organic.elis.ugent.be/files/PracticalESN.pdf][Paper PDF]]

**** TODO A tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the "echo state network" approach - Jaeger 2002
     [[http://www.pdx.edu/sites/www.pdx.edu.sysc/files/Jaeger_TrainingRNNsTutorial.2005.pdf][Paper PDF]]

**** TODO Echo-State Network in Java - Lohmann 2010
     [[http://www.uni-tuebingen.de/index.php?eID=tx_nawsecuredl&u=0&g=0&t=1429926389&hash=8b6ef6bdb4a6eb32d3871b596e14efd92f7e15f7&file=fileadmin/Uni_Tuebingen/Fakultaeten/InfoKogni/WSI/KogModel/ext00209/user_upload/Publications/2010/ESNJava1.0Report.pdf][Paper PDF]]

- ESNs start with a "washout" phase to overcome random oscillations in the reservoir. Coherent oscillations form that based on input/output?

**** TODO Memory capacity of input-driven Echo State Networks at the Edge of Chaos - Barancok 2014
     [[../papers/barancok2014.pdf][Local PDF]]
**** TODO Multi-reservoir Echo State Network with Sparse Bayesian Learning - Han 2010
     [[../papers/han2010.pdf][Local PDF]]
**** DONE Echo State Networks for Multi-dimensional Data Clustering - Koprinkova 2012
     CLOSED: [2015-05-02 Sat 19:37]
     [[../papers/koprinkova2012.pdf][Local PDF]]
**** DONE Echo State Networks in Dynamic Data Clustering - Koprinkova 2013
     CLOSED: [2015-05-02 Sat 19:37]
     [[../papers/koprinkova2013.pdf][Local PDF]]
**** TODO Stable training method for ESNs running in closed-loop based on particle swarm optimization algorithm - Song 2009
     [[../papers/song2009.pdf][Local PDF]]
** Networks
*** DONE MASSEXODUS: modeling evolving networks in harsh environments - Navlakha, Bar-Joseph 2015
   CLOSED: [2015-02-27 Fri 21:38]
   [[http://www.snl.salk.edu/~navlakha/pubs/pkdd2015.pdf][Paper PDF]]
   [[http://link.springer.com/article/10.1007/s10618-014-0399-1][Citation]]
   
   "biological networks are shaped by their environments"

   "For both social and biological networks, node loss occurs alongside an underlying growth process."

*** TODO Make It or Break It: Manipulating Robustness in Large Networks - Chan 2014
   [[http://www3.cs.stonybrook.edu/~leman/pubs/14-sdm-miobi.pdf][Paper PDF]]
   [[http://epubs.siam.org/doi/abs/10.1137/1.9781611973440.37][Citation]]

** Dynamical Systems
*** DONE Computer systems are dynamical systems
   CLOSED: [2015-03-02 Mon 21:09]
   [[http://www-plan.cs.colorado.edu/klipto/CHAOEH193033124_2.pdf][Paper PDF]]
   [[http://scitation.aip.org/content/aip/journal/chaos/19/3/10.1063/1.3187791][Citation]]

   "While the program dynaics are generally simple and easy to understand, the performance dynamics of a program running on a modern computer can be complex and even chaotic."

   "It is important to note that not all of this dynamical complexity and richness manifest unless one studies a real computer, not just a simulator that mimics its behavior - the common approach in previous work on this topic in both the computer architecture and dynamical systems literature."

Must look at an implemented computer and not just the abstraction of it. 

   "In the broader picture, our results suggest that one cannot understand the behavior of a computer by understanding how the hardware and software subsystems function and then composing their dynamics. Instead, one must treat the system as a network of complex, nonlinear, interacting parts - CPU, cache, RAM, disk, graphics cards, operating system, user programs, etc. - and analyze the resulting dynamics as a whole."

   "the dynamics of a computer system depends both on the ISA and on the implementation. Indeed... the same software can produce chaotic behavior on a computer that is built around one microprocessor and periodic behavior on a computer that is built around another processor, even if both follow the x86 specification."

   "The time scale for the discrete-time dynamics is imposed by the internal clock on the chip. Designers intentionally choose the clock cycle to be larger than the time scales of the continuous dynamics in order to ensure that the discrete-time dynamics dominates the behavior of the system."

   "Modern computer hardware is composed of many tightly coupled subsystems, however; the execution unit of the computer proceeds only when it receives data from the local cache, for instance. While the number of transistors in the system is extremely large, this coupling - as in other dynamical systems - reduces the effective dimensionality of the system (cf. millions of planetesimals coalescing into a single rigid body). We conjecture that the coupling of subsystems in a computer is responsible for the low-dimensional dynamics observed here."

   "... the topological dimension of the two systems' state spaces is similar, even though the dynamics of their trajetories is different. This point is particularly interesting in view of the enormity of the potential state space and the differences that we have noted about these two processors. The similarity in our estimates of the state-space dimension is likely because the dynamics are dominated by the memory subsystem of the computer."

   "The nature of the dynamics changed completely when the same program was run on a computer that uses a different Intel processor, even though the design of that processor adhered to the same specification, affirming the role of implementation in the dynamics. Changing the program also changed the dynamics, verifying the presence of implementation dynamics code in the model of Eq. 2. When the two programs were interleaved in time, the dynamics alternated accordingly, leading us to a view of a computer as a dynamical system under the influence of a periodic series of externally forced bifurcations. All of these experiments have been repeated on multiple machines under different external operating conditions while maintaining internal conditions constant insofar as possible."

It seems that computers have at a low level dynamical properties that are also seen in the brain, but for the high level operation of the system, the digital computer seeks to mask those dynamics by imposing a more predictable overt behavior. The nonlinearities are masked instead of being exploited. The brain on the other hand uses those low-level nonlinearities to give the system self-organizing control and even nonlinear overt behavior. The difference is in how the systems deal with the low-level dynamics, not the presence or absence of the dynamics in itself.

* Tutorials
** [[http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial][UFLDL Tutorial]]
*** DONE Exercise: Sparse Autoencoder
   CLOSED: [2015-02-11 Wed 17:47]
*** DONE Exercise: Vectorization
   CLOSED: [2015-02-11 Wed 17:47]
*** DONE Exercise: PCA and Whitening
   CLOSED: [2015-02-11 Wed 17:47]
*** DONE Exercise: Softmax Regression
   CLOSED: [2015-02-11 Wed 17:47]
*** DONE Exercise: Implement deep networks for digit classification
    CLOSED: [2015-02-22 Sun 11:22]
*** DONE debug stackedAE, getting ZERO gradients
    CLOSED: [2015-02-16 Mon 18:06]
*** TODO rerun entire exercise starting at train second sparse AE
*** TODO Exercise: Learning color features with Sparse Autoencoders
*** TODO Exercise: Convolution and Pooling
** [[http://deeplearning.net/software/theano/tutorial/][Theano Tutorial]]
*** DONE Python tutorial
    CLOSED: [2015-02-22 Sun 11:56]
*** DONE NumPy refresher
    CLOSED: [2015-02-22 Sun 11:56]
**** DONE Matrix conventions for machine learning
     CLOSED: [2015-02-22 Sun 11:56]
**** DONE Broadcasting
     CLOSED: [2015-02-22 Sun 11:56]
*** DONE Baby Steps - Algebra
    CLOSED: [2015-02-22 Sun 11:56]
**** DONE Adding two Scalars
     CLOSED: [2015-02-22 Sun 11:56]
**** DONE Adding two Matrices
     CLOSED: [2015-02-22 Sun 11:56]
**** DONE Exercise
     CLOSED: [2015-02-22 Sun 11:56]
*** DONE More Examples
    CLOSED: [2015-02-22 Sun 11:56]
**** DONE Logistic Function
     CLOSED: [2015-02-22 Sun 11:56]
**** DONE Computing More than one Thing at the Same Time
     CLOSED: [2015-02-22 Sun 11:56]
**** DONE Setting a Default Value for an Argument
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Using Shared Variables
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Using Random Numbers
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Brief Example
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Seeding Streams
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Sharing Streams Between Functions
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Copying Random State Between Theano Graphs
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Other Random Distributions
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Other Implementations
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE A Real Example: Logistic Regression
     CLOSED: [2015-02-22 Sun 11:57]
*** DONE Graph Structures
    CLOSED: [2015-02-24 Tue 17:35]
**** DONE Theano Graphs
     CLOSED: [2015-02-24 Tue 17:35]
**** DONE Automatic Differentiation
     CLOSED: [2015-02-24 Tue 17:35]
**** DONE Optimizations
     CLOSED: [2015-02-24 Tue 17:35]
*** DONE Printing/Drawing Theano graphs
    CLOSED: [2015-02-24 Tue 17:39]
**** DONE Pretty Printing
     CLOSED: [2015-02-24 Tue 17:39]
**** DONE Debug Printing
     CLOSED: [2015-02-24 Tue 17:39]
**** DONE Picture Printing
     CLOSED: [2015-02-24 Tue 17:39]
*** DONE Derivatives in Theano
    CLOSED: [2015-02-24 Tue 19:16]
**** DONE Computing Gradients
     CLOSED: [2015-02-24 Tue 19:00]
**** DONE Computing the Jacobian
     CLOSED: [2015-02-24 Tue 19:00]
**** DONE Computing the Hessian
     CLOSED: [2015-02-24 Tue 19:00]
**** DONE Jacobian times a Vector
     CLOSED: [2015-02-24 Tue 19:00]
***** DONE R-operator
      CLOSED: [2015-02-24 Tue 19:12]
***** DONE L-operator
      CLOSED: [2015-02-24 Tue 19:12]
**** DONE Hessian times a Vector
     CLOSED: [2015-02-24 Tue 19:16]
**** DONE Final Pointers
     CLOSED: [2015-02-24 Tue 19:16]
*** TODO Configuration Settings and Compiling Modes
**** TODO Configuration
**** TODO Exercise
**** TODO Mode
**** TODO Linkers
**** TODO Using DebugMode
**** TODO ProfileMode
***** TODO Creating a ProfileMode Instance
***** TODO Compiling your Graph with ProfileMode
***** TODO Retrieving Timing Information
*** TODO Loading and Saving
**** TODO The Basics of Pickling
**** TODO Short-Term Serialization
**** TODO Long-Term Serialization
*** TODO Conditions
**** TODO IfElse vs Switch
*** TODO Loop
**** TODO Scan
**** TODO Exercise
*** TODO Sparse
**** TODO Compressed Sparse Format
***** TODO Which format should I use?
**** TODO Handling Sparse in Theano
***** TODO To and Fro
***** TODO Properties and Construction
***** TODO Structured Operation
***** TODO Gradient
*** TODO Using the GPU
**** TODO CUDA backend
***** TODO Testing Theano with GPU
***** TODO Returning a Handle to Device-Allocated Data
***** TODO What Can Be Accelerated on the GPU
***** TODO Tips for Improving Performance on GPU
***** TODO GPU Async capabilities
***** TODO Changing the Value of Shared Variables
****** TODO Exercise
**** TODO GpuArray Backend
***** TODO Testing Theano with GPU
***** TODO Returning a Handle to Device-Allocated Data
***** TODO What Can be Accelerated on the GPU
***** TODO GPU Async Capabilities
**** TODO Software for Directly Programming a GPU
**** TODO Learning to Program with PyCUDA
***** TODO Exercise
***** TODO Exercise
*** TODO PyCUDA/CUDAMat/Gnumpy compatibility
**** TODO PyCUDA
***** TODO Transfer
***** TODO Compiling with PyCUDA
***** TODO Theano Op using a PyCUDA function
**** TODO CUDAMat
**** TODO Gnumpy
*** TODO Understanding Memory Aliasing for Speed and Correctness
**** TODO The Memory Model: Two Spaces
**** TODO Borrowing when Creating Shared Variables
**** TODO Borrowing when Accessing Value of Shared Variables
***** TODO Retrieving
***** TODO Assigning
**** TODO Borrowing when Constructing Function Objects
*** TODO How Shape Information is Handled by Theano
**** TODO Shape Inference Problem
**** TODO Specifing Exact Shape
**** TODO Future Plans
*** TODO Debugging Theano: FAQ and Troubleshooting
**** TODO Isolating the Problem/Testing Theano Compiler
**** TODO Interpreting Error Messages
**** TODO Using Test Values
**** TODO “How do I Print an Intermediate Value in a Function/Method?”
**** TODO “How do I Print a Graph?” (before or after compilation)
**** TODO “The Function I Compiled is Too Slow, what’s up?”
**** TODO “How do I Step through a Compiled Function?”
**** TODO How to Use pdb
**** TODO Dumping a Function to help debug
*** TODO Profiling Theano function
*** TODO Extending Theano
**** TODO Theano Graphs
**** TODO Op Structure
**** TODO Op Example
**** TODO How To Test it
***** TODO Basic Tests
***** TODO Testing the infer_shape
***** TODO Testing the gradient
***** TODO Testing the Rop
***** TODO Testing GPU Ops
**** TODO Running Your Tests
***** TODO theano-nose
***** TODO nosetests
***** TODO In-file
**** TODO Exercise
**** TODO as_op
***** TODO as_op Example
***** TODO Exercise
**** TODO Random numbers in tests
**** TODO Documentation
**** TODO Final Note
*** TODO Extending Theano with a C Op
**** TODO Python C-API
***** TODO Reference counting
**** TODO NumPy C-API
***** TODO NumPy data types
***** TODO NumPy ndarrays
***** TODO Accessing NumPy ndarrays’ data and properties
***** TODO Creating NumPy ndarrays
**** TODO Methods the C Op needs to define
**** TODO Simple C Op example
**** TODO More complex C Op example
**** TODO Alternate way of defining C Ops
***** TODO Main function
***** TODO Macros
***** TODO Support code
**** TODO Final Note
*** TODO Python Memory Management
**** TODO Basic Objects
**** TODO Internal Memory Management
**** TODO Pickle
*** TODO Multi cores support in Theano
**** TODO BLAS operation
**** TODO Parallel element wise ops with OpenMP
** DONE [[http://mdp-toolkit.sourceforge.net/tutorial/tutorial.html][MDP Tutorial]]
   CLOSED: [2015-04-01 Wed 20:32]
** DONE [[http://organic.elis.ugent.be/node/267][Oger Tutorial]]
   CLOSED: [2015-04-01 Wed 20:53]
** DONE [[http://gitimmersion.com/][Git Immersion]]
   CLOSED: [2015-04-17 Fri 11:25]
* Videos
*** Theano
**** DONE [[https://www.youtube.com/watch?v=S75EdAcXHKk][Introduction to Deep Learning with Python]]
     CLOSED: [2015-02-28 Sat 13:08]
** TODO [[https://www.youtube.com/watch?v=W15K9PegQt0][Andres Ng: Machine Learning via Larg-scale Brain Simulations]]
** TODO [[https://www.youtube.com/user/aicourses/playlists?view=50&sort=dd&shelf_id=2][Neural Networks for ML]]
*** DONE [[https://www.youtube.com/playlist?list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2][06 Optimization]]
    CLOSED: [2015-02-28 Sat 17:20]
**** DONE [[https://www.youtube.com/watch?v=GvHmwBc9N30&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=1][Overview of Mini Batch Gradient Descent]]
     CLOSED: [2015-02-28 Sat 16:18]
**** DONE [[https://www.youtube.com/watch?v=-5Wa4O8r-xM&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=2][A Bag of Tricks for Mini Batch Gradient Descent]]
     CLOSED: [2015-02-28 Sat 16:18]
**** DONE [[https://www.youtube.com/watch?v=8yg2mRJx-z4&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=3][The Momentum Method]]
     CLOSED: [2015-02-28 Sat 16:26]
**** DONE [[https://www.youtube.com/watch?v=u8dHl8De-cc&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=4][Adaptive Learning Rates for Each Connection]]
     CLOSED: [2015-02-28 Sat 17:19]
**** DONE [[https://www.youtube.com/watch?v=LGA-gRkLEsI&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=5][Rmsprop Divide the Gradient by a Running Average of it Recent Magnitude]]
     CLOSED: [2015-02-28 Sat 17:20]
*** TODO [[https://www.youtube.com/playlist?list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx][07 Recurrent Neural Networks]]
**** TODO [[https://www.youtube.com/watch?v=lKDfBZz7Wy8&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=1][Modeling Sequences a Brief Overview]] 
**** TODO [[https://www.youtube.com/watch?v=gPdbTIEMQwY&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=2][Training RNNs with Back Propagation]]
**** TODO [[https://www.youtube.com/watch?v=z61VFeALk3o&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=3][A Toy Example of Training an RNN]]
**** TODO [[https://www.youtube.com/watch?v=Pp4oKq4kCYs&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=4][Why it is Difficult to Train an RNN]] 
**** TODO [[https://www.youtube.com/watch?v=lsV5rFbs-K0&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=5][Long Term Short Term Memory]]
* Research with Peter  
** Notes

   Precision: fraction of asserted classifications that are actually correct
   Recall: fraction of total instances that were classified correctly
   F1 score: a weighted average of precision and recall that measures accuracy, in range [0,1]
** List of causal words

because
caus(es, ing, ally, ation)
due to
since
as
for
thus
therefor
result(s, ing)
when
is (are)
by
depend(s, ing, ent)
if/then

** Mikolov word vectors:

Maybe this kind of algebraic reasoning could be used to uncover causal relationships? That is, suppose you built up vector representations of your corpus such that you had a well-defined cause and effect relationship established. For example, you had a relationship "pressure increases as volume decreases". If you had compositional representations, then you could represent this in your vector space as V = vector("pressure increases") + vector("volume decreases"). If this is a cause-effect sentence based on the corpus you trained on, then you should be able to V - vector("cause") = vector("pressure increases") which would be the left over effect in the sentence. That seems to presuppose the knowledge of the causal relationship, but if your corpus contained a lot of instances of cause effect relationships, especially when established by an explicit causal connective, then maybe that would come out. 

You could maybe build up a notion of "cause" by performing some computation on words with a high similarity to "because", so then vector("cause") would be something meaningful to perform algebraic operations on no matter the causal connective actually used (because, since, as, for, thus, therefore, etc.). Presumably this kind of similarity would come out of the corpus training, and perhaps you could augment it by training on a large corpus of argumentative text (to learn the causal relations in itself) and then incorporate into your domain corpus.

Phrase composition seems to need very large data sets (they used 33 billion words) to be successful. BUT, they were using just effectively random text and learning very general concepts. Perhaps if you are operating only in a specific domain and are using training data that involves the same content but phrased in many unique ways, it might be able to pick up on phrases in that particular domain well. And generally that seems the only means by which this approach will be successful in the causality domain without resorting to an enormous training set - there will be a lot of content redundancy in the training data, but it will all be about the desired topic and so it can pick up on the causal relations within that domain. To extrapolate to other domains, will have to train on other domains, and to make this feasible in domain-free setting (i.e the domain is unknown), will likely have to train vector representations on an enormous corpus rich with causal relationships.
