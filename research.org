* Books
** Neural Network Architectures - Judith Dayhoff
   [[http://books.google.com/books/about/Neural_network_architectures.html?id=9RwnAAAAMAAJ][Google Books]]
 
#+BEGIN_QUOTE
(p12) "Neural network architectures encode information in a distributed fashion. Typically the information that is stored in a neural net is shared by many of its processing units. This type of coding is in stark contrast to traditional memory schemes, where particular pieces of information are stored in particular locations of memory. Traditional speech recognition systems, for example, contain a lookup table of template speech patterns (individual syllables or words) that are compared one by one to spoken inputs. Such templates are stored in a specific location of the computer memory. Neural networks, in contrast, identify spoken syllables by using a number of processing units simultaneously. The internal representation is thus distributed across all or part of the network. Furthermore, more than one syllable or pattern may be stored at the same time by the same network.

    Distributed storage schemes provide many advantages, the most important being that the information representation can be redundant. Thus a neural network system can undergo partial destruction of the network and may still be able to function correctly. Although redundancy can be buit into other types of systems, the neural network has a natural way to organize and implement this redundancy; the result is a naturally fault- or err-tolerant system."
#+END_QUOTE

    Neural networks are not unlike self-organizing agent systems. Each node can be considered as an agent that is very simply programmed (responds to stimulus from the environment by receiving activations from incoming connections and using its assigned function to send an output to other agents) and collectively they are performing a desired global task. They are self-organizing in that they learn to communicate the appropriate values between one another that results in the system being able to perform well. None of the agents have knowledge of the global task or the state of the rest of the network, all communication is local (with perhaps some further reaching connections being permitted, e.g. long range feedback or the concatenation approach to handling autoencoders.

#+BEGIN_QUOTE
(p39) "Mathematical analysis has shown that when this equality is true (bi-directional weights equal), the network is able to converge - that is, it eventually attains a stable state. Convergence of the network is necessary in order for it to perform useful computational tasks such as optimizatino and associative memory. Many networks with unqual weights (w_ij != w_ji) also converge successfully.
#+END_QUOTE

#+BEGIN_QUOTE
(p42) "The randomized updates used in the Hopfield network provide an important difference between it and other paradigms. Most other neural network paradigms have a layer of processing units updated at the same time (or nearly the same time). In contrast, the asynchronous updating of the Hopfield Net is a closer match to biological reality - biological neurons update their own states due to events that impinge upon the neuron. These impinging events are not synchronized from neuron to neuron."
#+END_QUOTE

#+BEGIN_QUOTE
(p56) "The nonsynchronous updating of processing units in the Hopfield network is a unique property, and is especially relevant to the study of biological systems becuase of their asynchronous updating of nerve cell attributes. In addition, asynchronous updating can be helpful in designing fast hardware implementations.

The most general advantage of the Hopfield network is its inherently parallel architecture. As a result, potential hardware implementations may be very fast. Tradeoffs must be assessed, however, between the size and speed of the network and the size of the applications problems."
#+END_QUOTE

*** Architecture Types
  + Perceptron
    - 0 or 1 thresholded output
    - Real-valued weights.
    - Only one *trainable* layer of weights. Possible to have multiple layers, but additional layers beyond the one trainable layer will be fixed weights.
    - A weight only changes if its corresponding training input is non-zero and there is a difference between target and output values.
  + Adaline/Madaline (_Ada_ptive _Li_near _Ne_uron)
    - Adaline has (+/-)1 thresholded output.
    - Real-valued weights.
    - Madaline is a system of connected adalines: layer of adalines, each adaline connected to an output unit.
    - Majority vote of adalines determines output of madeline: more than half of adalines have +1(or -1), so does the madaline.
    - Weight update: adaline unit whose sum was closest to 0 in the wrong direction is updated. "the adaptiation rule assigns responsibility to the unit that can most easily assume it. Only one adaline unit updates its weights; the others keep their weights the same"
  + Hopfield Network
    - Fully interconnected network of units (complete digraph)
    - Each unit binary 0 or 1 (alternatively -1 or 1)
    - Network state represented by bit vector
    - With weights w_ij = w_ji, network converges to a stable state (all units left unchanged after update), *necessary to perform useful computational tasks*
    - Update one unit at a time, continue until no more changes are possible
    - Can update units in sequence; Hopfield chose at random to ensure same average update rate
    - [[http://cns.upf.edu/jclub/hopfield82.pdf][Hopfield (1982)]] proves network will reach a stable state.
    - Cannot reach global minimum of "Energy" function from local minimum (Boltzmann machine overcomes this with noise to shake it out)
    - [[http://www.pnas.org/content/81/10/3088.full.pdf][Hopfield (1984)]] extended the network to have continuous unit values (using nonlinear sigmoid function)
    - Traveling Salesman: can represent tours as NxN matrix with one 1 in each column and row, corresponding to one city at a time being visited only once. Energy function has multiple terms, each term imposing a constraint of the problem. Hopfield had success with <30 cities but it was shown later that the approach does not scale to larger problem spaces.
  + Back-propagation
    - Input layer, output layer, one or more hidden layers. 
    - A layer is often fully interconnected to the next layer in the stack, but not necessarily
    - No a priori knowledge of a mathematical function mapping input patterns to output patterns is needed; self-organization of the weights finds the mapping
    - Nonlinear activation function for hidden layer provides "soft" threshold
    - Standard sigmoid has "interesting" values for range [-3,3], i.e. values increase monotonically from 0 to 1 with a sharp increase around x=0; asymptotically goes to 0 for x < -3 and to 1 for x > 3
    - Sigmoid can be shifted left/right by adding/subtracting constant value - this is what the bias nodes do
    - In the error deltas for the output units \delta_j = (t_j - a_j)f^\prime(S_j), the f^\prime forces a stronger correction when S_j is near the rapid rise of the sigmoid (sigmoid derivative is a bell-shaped curve centered at 0)
    - In weight update \Delta w_{ji} = \eta \delta_j a_i, larger error \delta_j results in larger adjustments to incoming weights, larger activation a_i of originating unit from lower layer results in larger weight adjustment.
    - Learning rate \eta (usually in range [0.25, 0.75] can cause network instability if too large, and very slow training if too small.
    - A RMSE value < 0.1 indicates the training set is learned
    - Techniques for avoiding local minima and speeding up convergence:
      * change the learning rate
      * simulated annealing: start with large learning rate and attenuate its value as training proceeds
      * change number of hidden nodes 
      * add noise to weights
      * momentum 
  + Competitive Learning
    - Two layers, input layer and competitive layer, layers are fully interconnected layers
    - (p96) "In the competitive layer, the units compete for the opportunity to respond to the input pattern."
    - Input units have activations of 0 or 1, weights are small in [0,1] and sum to 1
    - For winner take all scheme, unit in output layer with highest weighted sum has activation 1, all others 0
    - Only weights going to winner are updated, and are updated by \Delta w_{ji} = g(\frac{x_i}{m} - w_{ji}) for g learning parameter (usually in range [0.01, 0.3]) and m number of active input units
    - Weight incremented when corresponding unit has activation 1, decremented when 0
    - Update procedure retains weights summing to 1
    - Unsupervised for dividing input patterns into self-learned classes

* Papers
** DONE MASSEXODUS: modeling evolving networks in harsh environments - Navlakha, Bar-Joseph 2015
   CLOSED: [2015-02-27 Fri 21:38]
   [[http://www.snl.salk.edu/~navlakha/pubs/pkdd2015.pdf][Paper PDF]]
   [[http://link.springer.com/article/10.1007/s10618-014-0399-1][Citation]]
   
   "biological networks are shaped by their environments"

   "For both social and biological networks, node loss occurs alongside an underlying growth process."

** TODO Make It or Break It: Manipulating Robustness in Large Networks - Chan 2014
   [[http://www3.cs.stonybrook.edu/~leman/pubs/14-sdm-miobi.pdf][Paper PDF]]
   [[http://epubs.siam.org/doi/abs/10.1137/1.9781611973440.37][Citation]]

** TODO Computer systems are dynamical systems
   [[http://www-plan.cs.colorado.edu/klipto/CHAOEH193033124_2.pdf][Paper PDF]]
   [[http://scitation.aip.org/content/aip/journal/chaos/19/3/10.1063/1.3187791][Citation]]

   "While the program dynaics are generally simple and easy to understand, the performance dynamics of a program running on a modern computer can be complex and even chaotic."

   "It is important to note that not all of this dynamical complexity and richness manifest unless one studies a real computer, not just a simulator that mimics its behavior - the common approach in previous work on this topic in both the computer architecture and dynamical systems literature."

   Must look at an implemented computer and not just the abstraction of it. 

   "In the broader picture, our results suggest that one cannot understand the behavior of a computer by understanding how the hardware and software subsystems function and then composing their dynamics. Instead, one must treat the system as a network of complex, nonlinear, interacting parts - CPU, cache, RAM, disk, graphics cards, operating system, user programs, etc. - and analyze the resulting dynamics as a whole."

   "the dynamics of a computer system depends both on the ISA and on the implementation. Indeed... the same software can produce chaotic behavior on a cmoputer that is built around one microprocessor and periodic behavior on a computer that is built around another processor, even if both follow the x86 specification."

   "The time scale for the discrete-time dynamics is imposed by the internal clock on the chip. Designers intentionally choose the clock cycle to be larger than the time scales of the continuous dynamics in order to ensure that the discrete-time dynamics dominates the behavior of the system."

   "Modern computer hardware is composed of many tightly coupled subsystems, however; the execution unit of the computer proceeds only when it receives data from the local cache, for instance. While the number of transistors in the system is extremely large, this coupling - as in other dynamical systems - reduces the effective dimensionality of the system (cf. millions of planetesimals coalescing into a single rigid body). We conjecture that the coupling of subsystems in a computer is responsible for the low-dimensional dynamics observed here."

   "... the topological dimension of the two systems' state spaces is similar, even though the dynamics of their trajetories is different. This point is particularly interesting in view of the enormity of the potential state space and the differences that we have noted about these two processors. The similarity in our estimates of the state-space dimension is likely because the dynamics are dominated by the memory subsystem of the computer."

   "The nature of the dynamics changed completely when the same program was run on a computer that uses a different Intel processor, even though the design of that processor adhered to the same specification, affirming the role of implementation in the dynamics. Changing the program also changed the dynamics, verifying the presence of implementation dynamics code in the model of Eq. 2. When the two programs were interleaved in time, the dynamics alternated accordingly, leading us to a view of a computer as a dynamical system under the influence of a periodic series of externally forced bifurcations. All of these experiments have been repeated on multiple machines under different external operating conditions while maintaining internal conditions constant insofar as possible."

* Tutorials
** [[http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial][UFLDL Tutorial]]
*** DONE Exercise: Sparse Autoencoder
   CLOSED: [2015-02-11 Wed 17:47]
*** DONE Exercise: Vectorization
   CLOSED: [2015-02-11 Wed 17:47]
*** DONE Exercise: PCA and Whitening
   CLOSED: [2015-02-11 Wed 17:47]
*** DONE Exercise: Softmax Regression
   CLOSED: [2015-02-11 Wed 17:47]
*** DONE Exercise: Implement deep networks for digit classification
    CLOSED: [2015-02-22 Sun 11:22]
*** DONE debug stackedAE, getting ZERO gradients
    CLOSED: [2015-02-16 Mon 18:06]
*** TODO rerun entire exercise starting at train second sparse AE
*** TODO Exercise: Learning color features with Sparse Autoencoders
*** TODO Exercise: Convolution and Pooling
** [[http://deeplearning.net/software/theano/tutorial/][Theano Tutorial]]
*** DONE Python tutorial
    CLOSED: [2015-02-22 Sun 11:56]
*** DONE NumPy refresher
    CLOSED: [2015-02-22 Sun 11:56]
**** DONE Matrix conventions for machine learning
     CLOSED: [2015-02-22 Sun 11:56]
**** DONE Broadcasting
     CLOSED: [2015-02-22 Sun 11:56]
*** DONE Baby Steps - Algebra
    CLOSED: [2015-02-22 Sun 11:56]
**** DONE Adding two Scalars
     CLOSED: [2015-02-22 Sun 11:56]
**** DONE Adding two Matrices
     CLOSED: [2015-02-22 Sun 11:56]
**** DONE Exercise
     CLOSED: [2015-02-22 Sun 11:56]
*** DONE More Examples
    CLOSED: [2015-02-22 Sun 11:56]
**** DONE Logistic Function
     CLOSED: [2015-02-22 Sun 11:56]
**** DONE Computing More than one Thing at the Same Time
     CLOSED: [2015-02-22 Sun 11:56]
**** DONE Setting a Default Value for an Argument
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Using Shared Variables
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Using Random Numbers
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Brief Example
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Seeding Streams
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Sharing Streams Between Functions
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Copying Random State Between Theano Graphs
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Other Random Distributions
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE Other Implementations
     CLOSED: [2015-02-22 Sun 11:57]
**** DONE A Real Example: Logistic Regression
     CLOSED: [2015-02-22 Sun 11:57]
*** DONE Graph Structures
    CLOSED: [2015-02-24 Tue 17:35]
**** DONE Theano Graphs
     CLOSED: [2015-02-24 Tue 17:35]
**** DONE Automatic Differentiation
     CLOSED: [2015-02-24 Tue 17:35]
**** DONE Optimizations
     CLOSED: [2015-02-24 Tue 17:35]
*** DONE Printing/Drawing Theano graphs
    CLOSED: [2015-02-24 Tue 17:39]
**** DONE Pretty Printing
     CLOSED: [2015-02-24 Tue 17:39]
**** DONE Debug Printing
     CLOSED: [2015-02-24 Tue 17:39]
**** DONE Picture Printing
     CLOSED: [2015-02-24 Tue 17:39]
*** DONE Derivatives in Theano
    CLOSED: [2015-02-24 Tue 19:16]
**** DONE Computing Gradients
     CLOSED: [2015-02-24 Tue 19:00]
**** DONE Computing the Jacobian
     CLOSED: [2015-02-24 Tue 19:00]
**** DONE Computing the Hessian
     CLOSED: [2015-02-24 Tue 19:00]
**** DONE Jacobian times a Vector
     CLOSED: [2015-02-24 Tue 19:00]
***** DONE R-operator
      CLOSED: [2015-02-24 Tue 19:12]
***** DONE L-operator
      CLOSED: [2015-02-24 Tue 19:12]
**** DONE Hessian times a Vector
     CLOSED: [2015-02-24 Tue 19:16]
**** DONE Final Pointers
     CLOSED: [2015-02-24 Tue 19:16]
*** TODO Configuration Settings and Compiling Modes
**** TODO Configuration
**** TODO Exercise
**** TODO Mode
**** TODO Linkers
**** TODO Using DebugMode
**** TODO ProfileMode
***** TODO Creating a ProfileMode Instance
***** TODO Compiling your Graph with ProfileMode
***** TODO Retrieving Timing Information
*** TODO Loading and Saving
**** TODO The Basics of Pickling
**** TODO Short-Term Serialization
**** TODO Long-Term Serialization
*** TODO Conditions
**** TODO IfElse vs Switch
*** TODO Loop
**** TODO Scan
**** TODO Exercise
*** TODO Sparse
**** TODO Compressed Sparse Format
***** TODO Which format should I use?
**** TODO Handling Sparse in Theano
***** TODO To and Fro
***** TODO Properties and Construction
***** TODO Structured Operation
***** TODO Gradient
*** TODO Using the GPU
**** TODO CUDA backend
***** TODO Testing Theano with GPU
***** TODO Returning a Handle to Device-Allocated Data
***** TODO What Can Be Accelerated on the GPU
***** TODO Tips for Improving Performance on GPU
***** TODO GPU Async capabilities
***** TODO Changing the Value of Shared Variables
****** TODO Exercise
**** TODO GpuArray Backend
***** TODO Testing Theano with GPU
***** TODO Returning a Handle to Device-Allocated Data
***** TODO What Can be Accelerated on the GPU
***** TODO GPU Async Capabilities
**** TODO Software for Directly Programming a GPU
**** TODO Learning to Program with PyCUDA
***** TODO Exercise
***** TODO Exercise
*** TODO PyCUDA/CUDAMat/Gnumpy compatibility
**** TODO PyCUDA
***** TODO Transfer
***** TODO Compiling with PyCUDA
***** TODO Theano Op using a PyCUDA function
**** TODO CUDAMat
**** TODO Gnumpy
*** TODO Understanding Memory Aliasing for Speed and Correctness
**** TODO The Memory Model: Two Spaces
**** TODO Borrowing when Creating Shared Variables
**** TODO Borrowing when Accessing Value of Shared Variables
***** TODO Retrieving
***** TODO Assigning
**** TODO Borrowing when Constructing Function Objects
*** TODO How Shape Information is Handled by Theano
**** TODO Shape Inference Problem
**** TODO Specifing Exact Shape
**** TODO Future Plans
*** TODO Debugging Theano: FAQ and Troubleshooting
**** TODO Isolating the Problem/Testing Theano Compiler
**** TODO Interpreting Error Messages
**** TODO Using Test Values
**** TODO “How do I Print an Intermediate Value in a Function/Method?”
**** TODO “How do I Print a Graph?” (before or after compilation)
**** TODO “The Function I Compiled is Too Slow, what’s up?”
**** TODO “How do I Step through a Compiled Function?”
**** TODO How to Use pdb
**** TODO Dumping a Function to help debug
*** TODO Profiling Theano function
*** TODO Extending Theano
**** TODO Theano Graphs
**** TODO Op Structure
**** TODO Op Example
**** TODO How To Test it
***** TODO Basic Tests
***** TODO Testing the infer_shape
***** TODO Testing the gradient
***** TODO Testing the Rop
***** TODO Testing GPU Ops
**** TODO Running Your Tests
***** TODO theano-nose
***** TODO nosetests
***** TODO In-file
**** TODO Exercise
**** TODO as_op
***** TODO as_op Example
***** TODO Exercise
**** TODO Random numbers in tests
**** TODO Documentation
**** TODO Final Note
*** TODO Extending Theano with a C Op
**** TODO Python C-API
***** TODO Reference counting
**** TODO NumPy C-API
***** TODO NumPy data types
***** TODO NumPy ndarrays
***** TODO Accessing NumPy ndarrays’ data and properties
***** TODO Creating NumPy ndarrays
**** TODO Methods the C Op needs to define
**** TODO Simple C Op example
**** TODO More complex C Op example
**** TODO Alternate way of defining C Ops
***** TODO Main function
***** TODO Macros
***** TODO Support code
**** TODO Final Note
*** TODO Python Memory Management
**** TODO Basic Objects
**** TODO Internal Memory Management
**** TODO Pickle
*** TODO Multi cores support in Theano
**** TODO BLAS operation
**** TODO Parallel element wise ops with OpenMP
* Videos
*** Theano
**** DONE [[https://www.youtube.com/watch?v=S75EdAcXHKk][Introduction to Deep Learning with Python]]
     CLOSED: [2015-02-28 Sat 13:08]
** TODO [[https://www.youtube.com/watch?v=W15K9PegQt0][Andres Ng: Machine Learning via Larg-scale Brain Simulations]]
** TODO [[https://www.youtube.com/user/aicourses/playlists?view=50&sort=dd&shelf_id=2][Neural Networks for ML]]
*** DONE [[https://www.youtube.com/playlist?list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2][06 Optimization]]
    CLOSED: [2015-02-28 Sat 17:20]
**** DONE [[https://www.youtube.com/watch?v=GvHmwBc9N30&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=1][Overview of Mini Batch Gradient Descent]]
     CLOSED: [2015-02-28 Sat 16:18]
**** DONE [[https://www.youtube.com/watch?v=-5Wa4O8r-xM&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=2][A Bag of Tricks for Mini Batch Gradient Descent]]
     CLOSED: [2015-02-28 Sat 16:18]
**** DONE [[https://www.youtube.com/watch?v=8yg2mRJx-z4&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=3][The Momentum Method]]
     CLOSED: [2015-02-28 Sat 16:26]
**** DONE [[https://www.youtube.com/watch?v=u8dHl8De-cc&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=4][Adaptive Learning Rates for Each Connection]]
     CLOSED: [2015-02-28 Sat 17:19]
**** DONE [[https://www.youtube.com/watch?v=LGA-gRkLEsI&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=5][Rmsprop Divide the Gradient by a Running Average of it Recent Magnitude]]
     CLOSED: [2015-02-28 Sat 17:20]
*** TODO [[https://www.youtube.com/playlist?list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx][07 Recurrent Neural Networks]]
**** TODO [[https://www.youtube.com/watch?v=lKDfBZz7Wy8&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=1][Modeling Sequences a Brief Overview]] 
**** TODO [[https://www.youtube.com/watch?v=gPdbTIEMQwY&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=2][Training RNNs with Back Propagation]]
**** TODO [[https://www.youtube.com/watch?v=z61VFeALk3o&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=3][A Toy Example of Training an RNN]]
**** TODO [[https://www.youtube.com/watch?v=Pp4oKq4kCYs&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=4][Why it is Difficult to Train an RNN]] 
**** TODO [[https://www.youtube.com/watch?v=lsV5rFbs-K0&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=5][Long Term Short Term Memory]]
