<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1. Books</a>
<ul>
<li><a href="#sec-1-1">1.1. Neural Network Architectures - Judith Dayhoff</a>
<ul>
<li><a href="#sec-1-1-1">1.1.1. Thoughts</a></li>
<li><a href="#sec-1-1-2">1.1.2. Quotes</a></li>
<li><a href="#sec-1-1-3">1.1.3. Architecture Types</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-2">2. Papers</a>
<ul>
<li><a href="#sec-2-1">2.1. <span class="done DONE">DONE</span> MASSEXODUS: modeling evolving networks in harsh environments - Navlakha, Bar-Joseph 2015</a></li>
<li><a href="#sec-2-2">2.2. <span class="todo TODO">TODO</span> Make It or Break It: Manipulating Robustness in Large Networks - Chan 2014</a></li>
<li><a href="#sec-2-3">2.3. <span class="todo TODO">TODO</span> Computer systems are dynamical systems - Mytkowicz 2008</a></li>
</ul>
</li>
<li><a href="#sec-3">3. Tutorials</a>
<ul>
<li><a href="#sec-3-1">3.1. UFLDL Tutorial</a>
<ul>
<li><a href="#sec-3-1-1">3.1.1. <span class="done DONE">DONE</span> Exercise: Sparse Autoencoder</a></li>
<li><a href="#sec-3-1-2">3.1.2. <span class="done DONE">DONE</span> Exercise: Vectorization</a></li>
<li><a href="#sec-3-1-3">3.1.3. <span class="done DONE">DONE</span> Exercise: PCA and Whitening</a></li>
<li><a href="#sec-3-1-4">3.1.4. <span class="done DONE">DONE</span> Exercise: Softmax Regression</a></li>
<li><a href="#sec-3-1-5">3.1.5. <span class="done DONE">DONE</span> Exercise: Implement deep networks for digit classification</a></li>
<li><a href="#sec-3-1-6">3.1.6. <span class="done DONE">DONE</span> debug stackedAE, getting ZERO gradients</a></li>
<li><a href="#sec-3-1-7">3.1.7. <span class="todo TODO">TODO</span> rerun entire exercise starting at train second sparse AE</a></li>
<li><a href="#sec-3-1-8">3.1.8. <span class="todo TODO">TODO</span> Exercise: Learning color features with Sparse Autoencoders</a></li>
<li><a href="#sec-3-1-9">3.1.9. <span class="todo TODO">TODO</span> Exercise: Convolution and Pooling</a></li>
</ul>
</li>
<li><a href="#sec-3-2">3.2. Theano Tutorial</a>
<ul>
<li><a href="#sec-3-2-1">3.2.1. <span class="done DONE">DONE</span> Python tutorial</a></li>
<li><a href="#sec-3-2-2">3.2.2. <span class="done DONE">DONE</span> NumPy refresher</a></li>
<li><a href="#sec-3-2-3">3.2.3. <span class="done DONE">DONE</span> Baby Steps - Algebra</a></li>
<li><a href="#sec-3-2-4">3.2.4. <span class="done DONE">DONE</span> More Examples</a></li>
<li><a href="#sec-3-2-5">3.2.5. <span class="done DONE">DONE</span> Graph Structures</a></li>
<li><a href="#sec-3-2-6">3.2.6. <span class="done DONE">DONE</span> Printing/Drawing Theano graphs</a></li>
<li><a href="#sec-3-2-7">3.2.7. <span class="done DONE">DONE</span> Derivatives in Theano</a></li>
<li><a href="#sec-3-2-8">3.2.8. <span class="todo TODO">TODO</span> Configuration Settings and Compiling Modes</a></li>
<li><a href="#sec-3-2-9">3.2.9. <span class="todo TODO">TODO</span> Loading and Saving</a></li>
<li><a href="#sec-3-2-10">3.2.10. <span class="todo TODO">TODO</span> Conditions</a></li>
<li><a href="#sec-3-2-11">3.2.11. <span class="todo TODO">TODO</span> Loop</a></li>
<li><a href="#sec-3-2-12">3.2.12. <span class="todo TODO">TODO</span> Sparse</a></li>
<li><a href="#sec-3-2-13">3.2.13. <span class="todo TODO">TODO</span> Using the GPU</a></li>
<li><a href="#sec-3-2-14">3.2.14. <span class="todo TODO">TODO</span> PyCUDA/CUDAMat/Gnumpy compatibility</a></li>
<li><a href="#sec-3-2-15">3.2.15. <span class="todo TODO">TODO</span> Understanding Memory Aliasing for Speed and Correctness</a></li>
<li><a href="#sec-3-2-16">3.2.16. <span class="todo TODO">TODO</span> How Shape Information is Handled by Theano</a></li>
<li><a href="#sec-3-2-17">3.2.17. <span class="todo TODO">TODO</span> Debugging Theano: FAQ and Troubleshooting</a></li>
<li><a href="#sec-3-2-18">3.2.18. <span class="todo TODO">TODO</span> Profiling Theano function</a></li>
<li><a href="#sec-3-2-19">3.2.19. <span class="todo TODO">TODO</span> Extending Theano</a></li>
<li><a href="#sec-3-2-20">3.2.20. <span class="todo TODO">TODO</span> Extending Theano with a C Op</a></li>
<li><a href="#sec-3-2-21">3.2.21. <span class="todo TODO">TODO</span> Python Memory Management</a></li>
<li><a href="#sec-3-2-22">3.2.22. <span class="todo TODO">TODO</span> Multi cores support in Theano</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#sec-4">4. Videos</a>
<ul>
<li>
<ul>
<li><a href="#sec-4-0-1">4.0.1. Theano</a></li>
</ul>
</li>
<li><a href="#sec-4-1">4.1. <span class="todo TODO">TODO</span> Andres Ng: Machine Learning via Larg-scale Brain Simulations</a></li>
<li><a href="#sec-4-2">4.2. <span class="todo TODO">TODO</span> Neural Networks for ML</a>
<ul>
<li><a href="#sec-4-2-1">4.2.1. <span class="todo TODO">TODO</span> 06 Optimization</a></li>
<li><a href="#sec-4-2-2">4.2.2. <span class="todo TODO">TODO</span> 07 Recurrent Neural Networks</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>

# Books<a id="sec-1" name="sec-1"></a>

## Neural Network Architectures - Judith Dayhoff<a id="sec-1-1" name="sec-1-1"></a>

[Google Books](http://books.google.com/books/about/Neural_network_architectures.html?id=9RwnAAAAMAAJ)

### Thoughts<a id="sec-1-1-1" name="sec-1-1-1"></a>

Neural networks are not unlike self-organizing agent systems. Each node can be considered as an agent that is very simply programmed (responds to stimulus from the environment by receiving activations from incoming connections and using its assigned function to send an output to other agents) and collectively they are performing a desired global task. They are self-organizing in that they learn to communicate the appropriate values between one another that results in the system being able to perform well. None of the agents have knowledge of the global task or the state of the rest of the network, all communication is local (with perhaps some further reaching connections being permitted, e.g. long range feedback or the concatenation approach to handling autoencoders.

### Quotes<a id="sec-1-1-2" name="sec-1-1-2"></a>

12: 

> "Neural network architectures encode information in a distributed fashion. Typically the information that is stored in a neural net is shared by many of its processing units. This type of coding is in stark contrast to traditional memory schemes, where particular pieces of information are stored in particular locations of memory. Traditional speech recognition systems, for example, contain a lookup table of template speech patterns (individual syllables or words) that are compared one by one to spoken inputs. Such templates are stored in a specific location of the computer memory. Neural networks, in contrast, identify spoken syllables by using a number of processing units simultaneously. The internal representation is thus distributed across all or part of the network. Furthermore, more than one syllable or pattern may be stored at the same time by the same network.
> 
> Distributed storage schemes provide many advantages, the most important being that the information representation can be redundant. Thus a neural network system can undergo partial destruction of the network and may still be able to function correctly. Although redundancy can be buit into other types of systems, the neural network has a natural way to organize and implement this redundancy; the result is a naturally fault- or err-tolerant system."

39:

> "Mathematical analysis has shown that when this equality is true (bi-directional weights equal), the network is able to converge - that is, it eventually attains a stable state. Convergence of the network is necessary in order for it to perform useful computational tasks such as optimizatino and associative memory. Many networks with unqual weights (w<sub>ij</sub> != w<sub>ji</sub>) also converge successfully.

42:

> "The randomized updates used in the Hopfield network provide an important difference between it and other paradigms. Most other neural network paradigms have a layer of processing units updated at the same time (or nearly the same time). In contrast, the asynchronous updating of the Hopfield Net is a closer match to biological reality - biological neurons update their own states due to events that impinge upon the neuron. These impinging events are not synchronized from neuron to neuron."

56:

> "The nonsynchronous updating of processing units in the Hopfield network is a unique property, and is especially relevant to the study of biological systems becuase of their asynchronous updating of nerve cell attributes. In addition, asynchronous updating can be helpful in designing fast hardware implementations.
> 
> The most general advantage of the Hopfield network is its inherently parallel architecture. As a result, potential hardware implementations may be very fast. Tradeoffs must be assessed, however, between the size and speed of the network and the size of the applications problems."

### Architecture Types<a id="sec-1-1-3" name="sec-1-1-3"></a>

-   Perceptron
    -   0 or 1 thresholded output
    -   Real-valued weights.
    -   Only one **trainable** layer of weights. Possible to have multiple layers, but additional layers beyond the one trainable layer will be fixed weights.
    -   A weight only changes if its corresponding training input is non-zero and there is a difference between target and output values.
-   Adaline/Madaline (<sub>Ada</sub><sub>ptive</sub> \_Li<sub>near</sub> \_Ne<sub>uron</sub>)
    -   Adaline has (+/-)1 thresholded output.
    -   Real-valued weights.
    -   Madaline is a system of connected adalines: layer of adalines, each adaline connected to an output unit.
    -   Majority vote of adalines determines output of madeline: more than half of adalines have +1(or -1), so does the madaline.
    -   Weight update: adaline unit whose sum was closest to 0 in the wrong direction is updated. "the adaptiation rule assigns responsibility to the unit that can most easily assume it. Only one adaline unit updates its weights; the others keep their weights the same"
-   Hopfield Network
    -   Fully interconnected network of units (complete digraph)
    -   Each unit binary 0 or 1 (alternatively -1 or 1)
    -   Network state represented by bit vector
    -   With weights w<sub>ij</sub> = w<sub>ji</sub>, network converges to a stable state (all units left unchanged after update), **necessary to perform useful computational tasks**
    -   Update one unit at a time, continue until no more changes are possible
    -   Can update units in sequence; Hopfield chose at random to ensure same average update rate
    -   [Hopfield (1982)](http://cns.upf.edu/jclub/hopfield82.pdf) proves network will reach a stable state.
    -   Cannot reach global minimum of "Energy" function from local minimum (Boltzmann machine overcomes this with noise to shake it out)
    -   [Hopfield (1984)](http://www.pnas.org/content/81/10/3088.full.pdf) extended the network to have continuous unit values (using nonlinear sigmoid function)
    -   Traveling Salesman: can represent tours as NxN matrix with one 1 in each column and row, corresponding to one city at a time being visited only once. Energy function has multiple terms, each term imposing a constraint of the problem. Hopfield had success with <30 cities but it was shown later that the approach does not scale to larger problem spaces.

# Papers<a id="sec-2" name="sec-2"></a>

## DONE MASSEXODUS: modeling evolving networks in harsh environments - Navlakha, Bar-Joseph 2015<a id="sec-2-1" name="sec-2-1"></a>

[Paper PDF](http://www.snl.salk.edu/~navlakha/pubs/pkdd2015.pdf)
[Citation](http://link.springer.com/article/10.1007/s10618-014-0399-1)

"biological networks are shaped by their environments"

"For both social and biological networks, node loss occurs alongside an underlying growth process."

## TODO Make It or Break It: Manipulating Robustness in Large Networks - Chan 2014<a id="sec-2-2" name="sec-2-2"></a>

[Paper PDF](http://www3.cs.stonybrook.edu/~leman/pubs/14-sdm-miobi.pdf)
[Citation](http://epubs.siam.org/doi/abs/10.1137/1.9781611973440.37)

## TODO Computer systems are dynamical systems - Mytkowicz 2008<a id="sec-2-3" name="sec-2-3"></a>

[Paper PDF](http://www-plan.cs.colorado.edu/klipto/CHAOEH193033124_2.pdf)
[Citation](http://scitation.aip.org/content/aip/journal/chaos/19/3/10.1063/1.3187791)

# Tutorials<a id="sec-3" name="sec-3"></a>

## [UFLDL Tutorial](http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial)<a id="sec-3-1" name="sec-3-1"></a>

### DONE Exercise: Sparse Autoencoder<a id="sec-3-1-1" name="sec-3-1-1"></a>

### DONE Exercise: Vectorization<a id="sec-3-1-2" name="sec-3-1-2"></a>

### DONE Exercise: PCA and Whitening<a id="sec-3-1-3" name="sec-3-1-3"></a>

### DONE Exercise: Softmax Regression<a id="sec-3-1-4" name="sec-3-1-4"></a>

### DONE Exercise: Implement deep networks for digit classification<a id="sec-3-1-5" name="sec-3-1-5"></a>

### DONE debug stackedAE, getting ZERO gradients<a id="sec-3-1-6" name="sec-3-1-6"></a>

### TODO rerun entire exercise starting at train second sparse AE<a id="sec-3-1-7" name="sec-3-1-7"></a>

### TODO Exercise: Learning color features with Sparse Autoencoders<a id="sec-3-1-8" name="sec-3-1-8"></a>

### TODO Exercise: Convolution and Pooling<a id="sec-3-1-9" name="sec-3-1-9"></a>

## [Theano Tutorial](http://deeplearning.net/software/theano/tutorial/)<a id="sec-3-2" name="sec-3-2"></a>

### DONE Python tutorial<a id="sec-3-2-1" name="sec-3-2-1"></a>

### DONE NumPy refresher<a id="sec-3-2-2" name="sec-3-2-2"></a>


1.  DONE Matrix conventions for machine learning

2.  DONE Broadcasting

### DONE Baby Steps - Algebra<a id="sec-3-2-3" name="sec-3-2-3"></a>


1.  DONE Adding two Scalars

2.  DONE Adding two Matrices

3.  DONE Exercise

### DONE More Examples<a id="sec-3-2-4" name="sec-3-2-4"></a>


1.  DONE Logistic Function

2.  DONE Computing More than one Thing at the Same Time

3.  DONE Setting a Default Value for an Argument

4.  DONE Using Shared Variables

5.  DONE Using Random Numbers

6.  DONE Brief Example

7.  DONE Seeding Streams

8.  DONE Sharing Streams Between Functions

9.  DONE Copying Random State Between Theano Graphs

10. DONE Other Random Distributions

11. DONE Other Implementations

12. DONE A Real Example: Logistic Regression

### DONE Graph Structures<a id="sec-3-2-5" name="sec-3-2-5"></a>


1.  DONE Theano Graphs

2.  DONE Automatic Differentiation

3.  DONE Optimizations

### DONE Printing/Drawing Theano graphs<a id="sec-3-2-6" name="sec-3-2-6"></a>


1.  DONE Pretty Printing

2.  DONE Debug Printing

3.  DONE Picture Printing

### DONE Derivatives in Theano<a id="sec-3-2-7" name="sec-3-2-7"></a>


1.  DONE Computing Gradients

2.  DONE Computing the Jacobian

3.  DONE Computing the Hessian

4.  DONE Jacobian times a Vector

    
    1.  DONE R-operator
    
    2.  DONE L-operator

5.  DONE Hessian times a Vector

6.  DONE Final Pointers

### TODO Configuration Settings and Compiling Modes<a id="sec-3-2-8" name="sec-3-2-8"></a>

1.  TODO Configuration

2.  TODO Exercise

3.  TODO Mode

4.  TODO Linkers

5.  TODO Using DebugMode

6.  TODO ProfileMode

    1.  TODO Creating a ProfileMode Instance
    
    2.  TODO Compiling your Graph with ProfileMode
    
    3.  TODO Retrieving Timing Information

### TODO Loading and Saving<a id="sec-3-2-9" name="sec-3-2-9"></a>

1.  TODO The Basics of Pickling

2.  TODO Short-Term Serialization

3.  TODO Long-Term Serialization

### TODO Conditions<a id="sec-3-2-10" name="sec-3-2-10"></a>

1.  TODO IfElse vs Switch

### TODO Loop<a id="sec-3-2-11" name="sec-3-2-11"></a>

1.  TODO Scan

2.  TODO Exercise

### TODO Sparse<a id="sec-3-2-12" name="sec-3-2-12"></a>

1.  TODO Compressed Sparse Format

    1.  TODO Which format should I use?

2.  TODO Handling Sparse in Theano

    1.  TODO To and Fro
    
    2.  TODO Properties and Construction
    
    3.  TODO Structured Operation
    
    4.  TODO Gradient

### TODO Using the GPU<a id="sec-3-2-13" name="sec-3-2-13"></a>

1.  TODO CUDA backend

    1.  TODO Testing Theano with GPU
    
    2.  TODO Returning a Handle to Device-Allocated Data
    
    3.  TODO What Can Be Accelerated on the GPU
    
    4.  TODO Tips for Improving Performance on GPU
    
    5.  TODO GPU Async capabilities
    
    6.  TODO Changing the Value of Shared Variables
    
        1.  TODO Exercise

2.  TODO GpuArray Backend

    1.  TODO Testing Theano with GPU
    
    2.  TODO Returning a Handle to Device-Allocated Data
    
    3.  TODO What Can be Accelerated on the GPU
    
    4.  TODO GPU Async Capabilities

3.  TODO Software for Directly Programming a GPU

4.  TODO Learning to Program with PyCUDA

    1.  TODO Exercise
    
    2.  TODO Exercise

### TODO PyCUDA/CUDAMat/Gnumpy compatibility<a id="sec-3-2-14" name="sec-3-2-14"></a>

1.  TODO PyCUDA

    1.  TODO Transfer
    
    2.  TODO Compiling with PyCUDA
    
    3.  TODO Theano Op using a PyCUDA function

2.  TODO CUDAMat

3.  TODO Gnumpy

### TODO Understanding Memory Aliasing for Speed and Correctness<a id="sec-3-2-15" name="sec-3-2-15"></a>

1.  TODO The Memory Model: Two Spaces

2.  TODO Borrowing when Creating Shared Variables

3.  TODO Borrowing when Accessing Value of Shared Variables

    1.  TODO Retrieving
    
    2.  TODO Assigning

4.  TODO Borrowing when Constructing Function Objects

### TODO How Shape Information is Handled by Theano<a id="sec-3-2-16" name="sec-3-2-16"></a>

1.  TODO Shape Inference Problem

2.  TODO Specifing Exact Shape

3.  TODO Future Plans

### TODO Debugging Theano: FAQ and Troubleshooting<a id="sec-3-2-17" name="sec-3-2-17"></a>

1.  TODO Isolating the Problem/Testing Theano Compiler

2.  TODO Interpreting Error Messages

3.  TODO Using Test Values

4.  TODO “How do I Print an Intermediate Value in a Function/Method?”

5.  TODO “How do I Print a Graph?” (before or after compilation)

6.  TODO “The Function I Compiled is Too Slow, what’s up?”

7.  TODO “How do I Step through a Compiled Function?”

8.  TODO How to Use pdb

9.  TODO Dumping a Function to help debug

### TODO Profiling Theano function<a id="sec-3-2-18" name="sec-3-2-18"></a>

### TODO Extending Theano<a id="sec-3-2-19" name="sec-3-2-19"></a>

1.  TODO Theano Graphs

2.  TODO Op Structure

3.  TODO Op Example

4.  TODO How To Test it

    1.  TODO Basic Tests
    
    2.  TODO Testing the infer<sub>shape</sub>
    
    3.  TODO Testing the gradient
    
    4.  TODO Testing the Rop
    
    5.  TODO Testing GPU Ops

5.  TODO Running Your Tests

    1.  TODO theano-nose
    
    2.  TODO nosetests
    
    3.  TODO In-file

6.  TODO Exercise

7.  TODO as<sub>op</sub>

    1.  TODO as<sub>op</sub> Example
    
    2.  TODO Exercise

8.  TODO Random numbers in tests

9.  TODO Documentation

10. TODO Final Note

### TODO Extending Theano with a C Op<a id="sec-3-2-20" name="sec-3-2-20"></a>

1.  TODO Python C-API

    1.  TODO Reference counting

2.  TODO NumPy C-API

    1.  TODO NumPy data types
    
    2.  TODO NumPy ndarrays
    
    3.  TODO Accessing NumPy ndarrays’ data and properties
    
    4.  TODO Creating NumPy ndarrays

3.  TODO Methods the C Op needs to define

4.  TODO Simple C Op example

5.  TODO More complex C Op example

6.  TODO Alternate way of defining C Ops

    1.  TODO Main function
    
    2.  TODO Macros
    
    3.  TODO Support code

7.  TODO Final Note

### TODO Python Memory Management<a id="sec-3-2-21" name="sec-3-2-21"></a>

1.  TODO Basic Objects

2.  TODO Internal Memory Management

3.  TODO Pickle

### TODO Multi cores support in Theano<a id="sec-3-2-22" name="sec-3-2-22"></a>

1.  TODO BLAS operation

2.  TODO Parallel element wise ops with OpenMP

# Videos<a id="sec-4" name="sec-4"></a>

### Theano<a id="sec-4-0-1" name="sec-4-0-1"></a>

1.  DONE [Introduction to Deep Learning with Python](https://www.youtube.com/watch?v=S75EdAcXHKk)

2.  TODO Autoencoders in Theano

    1.  TODO [Reading Olhausen Natural Images](https://www.youtube.com/watch?v=Bygc0hX2t7M&list=PLDUHaKzf8LMZRnpsvhaa6MT46XZCUmbH7)
    
    2.  TODO [Generating Patches](https://www.youtube.com/watch?v=VZ9PwvgTXWY&index=2&list=PLDUHaKzf8LMZRnpsvhaa6MT46XZCUmbH7)
    
    3.  TODO [Preprocessing the Data](https://www.youtube.com/watch?v=FvCeOqpd4W8&list=PLDUHaKzf8LMZRnpsvhaa6MT46XZCUmbH7&index=3)
    
    4.  TODO [Defining the Model in Theano](https://www.youtube.com/watch?v=0OA5tp1yG6Q&list=PLDUHaKzf8LMZRnpsvhaa6MT46XZCUmbH7&index=4)
    
    5.  TODO [The Costs and the Training Function](https://www.youtube.com/watch?v=9YslqIFheBU&list=PLDUHaKzf8LMZRnpsvhaa6MT46XZCUmbH7&index=5)
    
    6.  TODO [Errata](https://www.youtube.com/watch?v=ecPwvkNtgY0&list=PLDUHaKzf8LMZRnpsvhaa6MT46XZCUmbH7&index=6)
    
    7.  TODO [The Main Loop and Visualization](https://www.youtube.com/watch?v=dZxvGGj65cc&list=PLDUHaKzf8LMZRnpsvhaa6MT46XZCUmbH7&index=7)

## TODO [Andres Ng: Machine Learning via Larg-scale Brain Simulations](https://www.youtube.com/watch?v=W15K9PegQt0)<a id="sec-4-1" name="sec-4-1"></a>

## TODO [Neural Networks for ML](https://www.youtube.com/user/aicourses/playlists?view=50&sort=dd&shelf_id=2)<a id="sec-4-2" name="sec-4-2"></a>

### TODO [06 Optimization](https://www.youtube.com/playlist?list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2)<a id="sec-4-2-1" name="sec-4-2-1"></a>

1.  TODO [Overview of Mini Batch Gradient Descent](https://www.youtube.com/watch?v=GvHmwBc9N30&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=1)

2.  TODO [A Bag of Tricks for Mini Batch Gradient Descent](https://www.youtube.com/watch?v=-5Wa4O8r-xM&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=2)

3.  TODO [The Momentum Method](https://www.youtube.com/watch?v=8yg2mRJx-z4&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=3)

4.  TODO [Adaptive Learning Rates for Each Connection](https://www.youtube.com/watch?v=u8dHl8De-cc&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=4)

5.  TODO [Rmsprop Divide the Gradient by a Running Average of it Recent Magnitude](https://www.youtube.com/watch?v=LGA-gRkLEsI&list=PLnnr1O8OWc6bAAkp43m0jNF_DEqwWp2o2&index=5)

### TODO [07 Recurrent Neural Networks](https://www.youtube.com/playlist?list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx)<a id="sec-4-2-2" name="sec-4-2-2"></a>

1.  TODO [Modeling Sequences a Brief Overview](https://www.youtube.com/watch?v=lKDfBZz7Wy8&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=1)

2.  TODO [Training RNNs with Back Propagation](https://www.youtube.com/watch?v=gPdbTIEMQwY&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=2)

3.  TODO [A Toy Example of Training an RNN](https://www.youtube.com/watch?v=z61VFeALk3o&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=3)

4.  TODO [Why it is Difficult to Train an RNN](https://www.youtube.com/watch?v=Pp4oKq4kCYs&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=4)

5.  TODO [Long Term Short Term Memory](https://www.youtube.com/watch?v=lsV5rFbs-K0&list=PLnnr1O8OWc6YM16tj9pdhBZOS9tDktNrx&index=5)